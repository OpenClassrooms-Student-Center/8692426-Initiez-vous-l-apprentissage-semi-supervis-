{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La propagation de labels, ou l'art de juger une image par ses voisins \n",
    "\n",
    "Bienvenue dans cette nouvelle exp√©rience ! Dans le chapitre pr√©c√©dent sur le pseudo-labeling, on a vu que notre mod√®le pouvait devenir un peu trop s√ªr de lui et finir par tourner en rond, en se confortant dans ses propres erreurs. C'est le fameux **biais de confirmation** ! \n",
    "\n",
    "> C'est comme ne parler qu'√† des gens qui sont d'accord avec vous : on n'apprend plus rien de nouveau.\n",
    "\n",
    "**Nos objectifs de super-d√©tective :**\n",
    "1.  **Recruter un expert** : Charger le mod√®le qu'on a p√©niblement entra√Æn√© au pseudo-labeling pour qu'il nous aide.\n",
    "2.  **Cartographier le terrain** : Utiliser cet expert pour extraire l'ADN de chaque image (ses *embeddings*).\n",
    "3.  **Tisser une toile** : Construire un graphe o√π chaque image est un n≈ìud, connect√© √† ses plus proches voisins.\n",
    "4.  **Laisser la magie op√©rer** : Regarder les √©tiquettes de nos 350 images connues se propager √† travers la toile pour deviner les autres.\n",
    "5.  **Comparer les r√©sultats** : Est-ce que cette m√©thode de 'sagesse des foules' est meilleure que de faire confiance √† un seul mod√®le ? Le suspense est √† son comble !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pr√©paration du terrain : on reprend (presque) les m√™mes !\n",
    "\n",
    "On commence par importer nos outils et pr√©parer notre jeu de donn√©es `DermaMNIST`. On va recr√©er notre sc√©nario de d√©part : 350 images √©tiquet√©es (50 par classe) et des milliers d'autres qui attendent d'√™tre identifi√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "\n",
    "# Pour la reproductibilit√©, parce qu'on est des gens s√©rieux\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille totale du jeu d'entra√Ænement : 7007 images\n",
      "Donn√©es √©tiquet√©es (nos indics ) : 350 images\n",
      "Donn√©es non-√©tiquet√©es (les myst√®res √† r√©soudre ) : 6657 images\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "data_flag = 'dermamnist'\n",
    "info = INFO[data_flag]\n",
    "n_classes = len(info['label'])\n",
    "n_channels = info['n_channels']\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# Transformations standard\n",
    "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])])\n",
    "\n",
    "# On charge le jeu d'entra√Ænement complet et le jeu de test\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=True)\n",
    "\n",
    "# On recr√©e notre situation de d√©part : 50 images par classe √©tiquet√©es, et le reste en attente\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "labels_array = np.array(train_dataset.labels).flatten()\n",
    "\n",
    "# S√©lectionner 50 images par classe\n",
    "labeled_indices = []\n",
    "for c in range(n_classes):\n",
    "    class_indices = np.where(labels_array == c)[0]\n",
    "    selected = np.random.choice(class_indices, min(50, len(class_indices)), replace=False)\n",
    "    labeled_indices.extend(selected)\n",
    "\n",
    "# Les indices non √©tiquet√©s sont le reste\n",
    "unlabeled_indices = list(set(all_indices) - set(labeled_indices))\n",
    "\n",
    "print(f'Taille totale du jeu d\\'entra√Ænement : {len(train_dataset)} images')\n",
    "print(f'Donn√©es √©tiquet√©es (nos indics ) : {len(labeled_indices)} images')\n",
    "print(f'Donn√©es non-√©tiquet√©es (les myst√®res √† r√©soudre ) : {len(unlabeled_indices)} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recruter notre expert : le mod√®le du chapitre pr√©c√©dent\n",
    "\n",
    "Pour que la propagation de labels fonctionne, on a besoin de 'sentir' la similarit√© entre les images. Utiliser les pixels bruts serait un d√©sastre ! \n",
    "\n",
    "On va donc faire appel √† un sp√©cialiste : le `SimpleCNN` qu'on a entra√Æn√© dans le notebook `P1C3`. M√™me s'il n'√©tait pas parfait, il a d√©j√† appris √† extraire des caract√©ristiques pertinentes des images de peau. On va lui demander de nous fournir les **embeddings** : une sorte de r√©sum√© num√©rique, ou d'ADN, pour chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© depuis : dermamnist_ssl_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1568, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On d√©finit l'architecture de notre CNN. \n",
    "# ATTENTION : Elle doit √™tre IDENTIQUE √† celle du mod√®le sauvegard√© !\n",
    "device = \"cpu\"\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        # Pour correspondre exactement au mod√®le de P1C3\n",
    "        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        if return_features:\n",
    "            return out  # Retourne les features avant la classification (dimension 1568)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = SimpleCNN(in_channels=n_channels, num_classes=n_classes)\n",
    "model_path = 'dermamnist_ssl_model.pth'\n",
    "\n",
    "try:\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    # Les noms de couches correspondent exactement, on charge tout\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f'‚úÖ Mod√®le charg√© depuis : {model_path}')\n",
    "except FileNotFoundError:\n",
    "    print(f'üö® Oups ! Le fichier {model_path} est introuvable.')\n",
    "    print('Veuillez d\\'abord ex√©cuter le notebook P1C3 pour entra√Æner et sauvegarder le mod√®le.')\n",
    "    raise\n",
    "\n",
    "# On passe le mod√®le sur le bon appareil et en mode √©valuation\n",
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extraction des 'coordonn√©es GPS' (embeddings) \n",
    "\n",
    "Maintenant que notre expert est pr√™t, on va le faire passer sur **toutes** les images de notre jeu d'entra√Ænement (√©tiquet√©es ou non) pour obtenir leurs fameux embeddings. C'est comme cr√©er une carte d'identit√© pour chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction des embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:03<00:00,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction termin√©e ! On a obtenu 7007 embeddings de dimension 1568.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(model, dataset, device):\n",
    "    \"\"\"Extrait les embeddings d'un dataset en utilisant un mod√®le.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=2)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(loader, desc='Extraction des embeddings'):\n",
    "            images = images.to(device)\n",
    "            feats = model(images, return_features=True)\n",
    "            embeddings.append(feats.cpu().numpy())\n",
    "            \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# On extrait les embeddings en utilisant notre mod√®le\n",
    "all_embeddings = get_embeddings(model, train_dataset, \"cpu\")\n",
    "\n",
    "print(f'\\nExtraction termin√©e ! On a obtenu {all_embeddings.shape[0]} embeddings de dimension {all_embeddings.shape[1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. La propagation des rumeurs (de labels) \n",
    "\n",
    "C'est le moment que vous attendiez tous ! On va utiliser l'algorithme `LabelSpreading` de scikit-learn.\n",
    "\n",
    "Comment √ßa marche ?\n",
    "1. Il prend tous nos embeddings et construit un graphe de similarit√© (notre fameuse toile ).\n",
    "2. On lui donne les 350 √©tiquettes qu'on conna√Æt. Pour les autres, on met une √©tiquette sp√©ciale : `-1` (qui veut dire 'Je ne sais pas').\n",
    "3. L'algorithme va alors 'propager' l'influence des √©tiquettes connues √† leurs voisins, puis aux voisins de leurs voisins, jusqu'√† ce que chaque image ait une √©tiquette probable.\n",
    "\n",
    "C'est un processus d√©mocratique o√π chaque image est influenc√©e par sa communaut√© !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification : 350 labels sont connus. Parfait !\n",
      "Propagation des labels en cours... C'est le moment d'aller prendre un caf√© \n",
      "Propagation termin√©e ! Voyons ce qu'on a trouv√©.\n"
     ]
    }
   ],
   "source": [
    "# On pr√©pare le tableau des labels pour l'algorithme\n",
    "labels_for_spreading = np.full(len(train_dataset), -1, dtype=int)\n",
    "labels_for_spreading[labeled_indices] = labels_array[labeled_indices]\n",
    "\n",
    "print(f'Verification : {np.sum(labels_for_spreading != -1)} labels sont connus. Parfait !')\n",
    "\n",
    "# On instancie le mod√®le LabelSpreading\n",
    "label_spreading_model = LabelSpreading(kernel='knn', n_neighbors=10, n_jobs=-1)\n",
    "\n",
    "print('Propagation des labels en cours... C\\'est le moment d\\'aller prendre un caf√© ')\n",
    "label_spreading_model.fit(all_embeddings, labels_for_spreading)\n",
    "print('Propagation termin√©e ! Voyons ce qu\\'on a trouv√©.')\n",
    "\n",
    "# On r√©cup√®re les labels pr√©dits pour l'ensemble du dataset\n",
    "predicted_labels = label_spreading_model.transduction_\n",
    "\n",
    "# On r√©cup√®re les probabilit√©s pr√©dites pour l'AUC\n",
    "predicted_probs = label_spreading_model.predict_proba(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Le verdict : alors, √ßa a march√© ? \n",
    "\n",
    "Le mod√®le a rempli tous les trous et a attribu√© une √©tiquette √† chaque image. Mais est-ce que ces pr√©dictions sont bonnes?\n",
    "\n",
    "Pour le savoir, on va comparer les √©tiquettes pr√©dites pour les donn√©es *initialement non-√©tiquet√©es* avec leurs vraies √©tiquettes (qu'on avait cach√©es). C'est l'heure de v√©rit√© !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Cr√©er un dataset personnalis√© avec les labels propag√©s\n",
    "class PropagatedDataset(Dataset):\n",
    "    def __init__(self, dataset, labels):\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]  # Ignore les labels d'origine, img est d√©j√† un tenseur transform√©\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# Cr√©er le nouveau dataset avec les labels propag√©s\n",
    "train_dataset_propagated = PropagatedDataset(train_dataset, predicted_labels)\n",
    "\n",
    "# Cr√©er un DataLoader pour l'entra√Ænement\n",
    "train_loader_propagated = DataLoader(train_dataset_propagated, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, epochs=10):\n",
    "    \"\"\"\n",
    "    Entra√Æne et √©value un mod√®le. Retourne (AUC, ACC, F1).\n",
    "    Si des listes globales metrics_auc/metrics_acc/metrics_f1 existent, y ajoute les scores.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.squeeze().long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # √âvaluation\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score_logits = torch.tensor([]).to(device)\n",
    "    y_score_preds = torch.tensor([]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            y_true = torch.cat((y_true, labels), 0)\n",
    "            y_score_logits = torch.cat((y_score_logits, outputs), 0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_score_preds = torch.cat((y_score_preds, preds), 0)\n",
    "\n",
    "    y_true = y_true.squeeze().cpu().numpy()\n",
    "    y_score_logits = y_score_logits.detach().cpu().numpy()\n",
    "    y_score_preds = y_score_preds.detach().cpu().numpy()\n",
    "\n",
    "    evaluator = Evaluator(data_flag, 'test')\n",
    "    auc, acc = evaluator.evaluate(y_score_logits)\n",
    "    f1 = f1_score(y_true, y_score_preds, average='macro')\n",
    "\n",
    "    try:\n",
    "        metrics_auc.append(auc)\n",
    "        metrics_acc.append(acc)\n",
    "        metrics_f1.append(f1)\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    print(f'AUC: {auc:.3f}, Accuracy: {acc:.3f}, F1: {f1:.3f}')\n",
    "    return (auc, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra√Ænement du mod√®le de base sur images √©tiquet√©es avec labels propag√©s...\n",
      "AUC: 0.504, Accuracy: 0.319, F1: 0.316\n",
      "Entra√Ænement termin√© !\n"
     ]
    }
   ],
   "source": [
    "# D√©finir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Entra√Ænement du mod√®le de base sur images √©tiquet√©es avec labels propag√©s...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "metrics = train_and_evaluate(model, train_loader_propagated, test_loader, optimizer, criterion)\n",
    "\n",
    "print('Entra√Ænement termin√© !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion et questions pour la suite \n",
    "\n",
    "Dans ce run, la propagation de labels sur des embeddings issus d‚Äôun petit `SimpleCNN` n‚Äôa pas surpass√© la boucle de pseudo‚Äëlabeling. C‚Äôest un r√©sultat fr√©quent quand les embeddings sont encore ¬´ jeunes ¬ª et que le graphe n‚Äôest pas optimis√©. Cela ne remet pas en cause l‚Äôint√©r√™t de la m√©thode‚ÄØ: la propagation reste une technique utile pour exploiter la structure globale des donn√©es et compl√©ter le pseudo‚Äëlabeling.\n",
    "\n",
    "La propagation de labels est puissante car elle exploite la **structure globale** des donn√©es, au lieu de se fier aux pr√©dictions isol√©es et parfois trop confiantes d'un seul mod√®le. \n",
    "\n",
    "**Mais on peut encore faire mieux ! Voici quelques questions pour ouvrir sur les prochains chapitres :**\n",
    "\n",
    "1. **La qualit√© des embeddings** : On a utilis√© un petit CNN entra√Æn√© sur peu de donn√©es. Que se passerait-il si on utilisait un mod√®le beaucoup plus puissant, comme un **ResNet pr√©-entra√Æn√© sur des millions d'images (ImageNet)**, pour extraire nos embeddings ? La carte serait-elle plus pr√©cise ?\n",
    "\n",
    "2. **Et si on cr√©ait de fausses images ?** On manque de donn√©es √©tiquet√©es. Et si, au lieu de deviner des labels, on demandait √† une IA de nous **g√©n√©rer de nouvelles images** de l√©sions cutan√©es qui ressemblent aux vraies ? C'est le monde fascinant des **GANs (Generative Adversarial Networks)** que nous explorerons bient√¥t !\n",
    "\n",
    "3. **Le meilleur des deux mondes ?** Peut-on combiner le pseudo-labeling et les approches par graphe ? (Indice : oui, et ce sont souvent les m√©thodes les plus performantes !)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
