{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced SSL Techniques - FixMatch, FlexMatch, and MixMatch\n",
    "\n",
    "Welcome to the advanced chapter of our SSL journey! We've explored pseudo-labeling and consistency regularization earlier. Now, let's dive into cutting-edge techniques: **FixMatch**, **FlexMatch**, and **MixMatch**. These methods combine the best of pseudo-labeling and consistency to tackle datasets with limited labels, like `DermaMNIST`.\n",
    "\n",
    "> Think of this as upgrading your SSL toolkit with turbocharged algorithms!\n",
    "\n",
    "**Core Principles:**\n",
    "- **FixMatch**: Uses weak and strong augmentations with a confidence threshold for pseudo-labels.\n",
    "- **FlexMatch**: Enhances FixMatch with dynamic thresholding per class, ideal for imbalanced data.\n",
    "- **MixMatch**: Adds data mixing (e.g., MixUp) to improve robustness by blending labeled and unlabeled samples.\n",
    "\n",
    "**Objectives:**\n",
    "1. Return to `DermaMNIST` classification with 100 labeled images.\n",
    "2. Implement FixMatch, FlexMatch, and MixMatch.\n",
    "3. Compare results to baseline methods to showcase SSL advancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation (The Usual Setup)\n",
    "\n",
    "Let‚Äôs set up our environment for `DermaMNIST` classification. We‚Äôll use 100 labeled images and leverage Albumentations for controlled augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled data: 500, Unlabeled data: 6507\n"
     ]
    }
   ],
   "source": [
    "# Load DermaMNIST data\n",
    "data_flag = 'dermamnist'\n",
    "info = INFO[data_flag]\n",
    "n_classes = len(info['label'])\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset = DataClass(split='train', download=True)\n",
    "test_dataset = DataClass(split='test', transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]), download=True)\n",
    "\n",
    "# Split into labeled (100) and unlabeled sets\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "labels_array = np.array(train_dataset.labels).flatten()\n",
    "labeled_indices, unlabeled_indices = train_test_split(all_indices, train_size=500, random_state=42, stratify=labels_array)\n",
    "\n",
    "print(f'Labeled data: {len(labeled_indices)}, Unlabeled data: {len(unlabeled_indices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Weak and Strong Augmentations\n",
    "\n",
    "We need two augmentation pipelines: weak for pseudo-label generation and strong for training robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quillaur/jupyter_notebooks/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define weak and strong augmentations for single-channel images\n",
    "weak_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(transpose_mask=True)  # Preserve 1 channel\n",
    "])\n",
    "\n",
    "strong_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(transpose_mask=True)  # Preserve 1 channel\n",
    "])\n",
    "print(\"Transforms initialized\")\n",
    "\n",
    "# Custom datasets for FixMatch\n",
    "class FixMatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img = np.array(img)  # Ensure img is [H, W] (single-channel)\n",
    "        transformed = self.transform(image=img)\n",
    "        return transformed['image'], torch.tensor(label).long()\n",
    "\n",
    "class FixMatchUnlabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, weak_transform, strong_transform):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = np.array(img)  # Ensure img is [H, W] (single-channel)\n",
    "        weak = self.weak_transform(image=img)['image']\n",
    "        strong = self.strong_transform(image=img)['image']\n",
    "        return weak, strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models and Training Loops\n",
    "\n",
    "We‚Äôll use a simple CNN and implement three training loops: FixMatch, FlexMatch, and MixMatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SimpleCNN model for single-channel input\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize model, optimizer, and loss functions\n",
    "model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "supervised_criterion = nn.CrossEntropyLoss()\n",
    "unsupervised_criterion = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.1 FixMatch Training Loop\n",
    "\n",
    "Let‚Äôs implement the FixMatch algorithm step-by-step.\n",
    "\n",
    "**Instructions:**\n",
    "1. Compute supervised loss on labeled data.\n",
    "2. Generate pseudo-labels: Predict on weak augmentations, compute probabilities, and create a mask for confident predictions (threshold = 0.95).\n",
    "3. Compute unsupervised loss: Predict on strong augmentations and apply the mask to confident pseudo-labels.\n",
    "4. Combine losses and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created: labeled=500, unlabeled=6507\n",
      "Dataloaders ready: batches labeled=32, unlabeled=102\n",
      "Starting training loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataLoaders\n",
    "labeled_dataset = FixMatchDataset(train_dataset, labeled_indices, strong_transform)\n",
    "unlabeled_dataset = FixMatchUnlabeledDataset(train_dataset, unlabeled_indices, weak_transform, strong_transform)\n",
    "print(f\"Datasets created: labeled={len(labeled_dataset)}, unlabeled={len(unlabeled_dataset)}\")\n",
    "\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=True)\n",
    "print(f\"Dataloaders ready: batches labeled={len(labeled_loader)}, unlabeled={len(unlabeled_loader)}\")\n",
    "\n",
    "print(\"Starting training loop\")\n",
    "# FixMatch training as a function (to unify with other methods)\n",
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, epochs=30, threshold=0.95, unsupervised_weight=1.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    sup_crit = nn.CrossEntropyLoss()\n",
    "    unsup_crit = nn.CrossEntropyLoss(reduction='none')\n",
    "    for epoch in tqdm(range(epochs), desc='Training FixMatch'):\n",
    "        model.train()\n",
    "        batch_iterator = zip(labeled_loader, unlabeled_loader)\n",
    "        for (labeled_imgs, labels), (weak_unlabeled, strong_unlabeled) in batch_iterator:\n",
    "            optimizer.zero_grad()\n",
    "            # Supervised loss\n",
    "            logits_sup = model(labeled_imgs)\n",
    "            loss_sup = sup_crit(logits_sup, labels.squeeze())\n",
    "            # Pseudo-labels from weak\n",
    "            with torch.no_grad():\n",
    "                logits_weak = model(weak_unlabeled)\n",
    "                probs = F.softmax(logits_weak, dim=1)\n",
    "                max_probs, pseudo_labels = torch.max(probs, dim=1)\n",
    "                mask = max_probs.ge(threshold).float()\n",
    "            # Unsupervised on strong\n",
    "            logits_strong = model(strong_unlabeled)\n",
    "            loss_unsup_raw = unsup_crit(logits_strong, pseudo_labels)\n",
    "            loss_unsup = (loss_unsup_raw * mask).mean()\n",
    "            # Total\n",
    "            total_loss = loss_sup + unsupervised_weight * loss_unsup\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FixMatch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training FixMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:33<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 50\n",
    "THRESHOLD = 0.95\n",
    "UNSUPERVISED_WEIGHT = 1.0\n",
    "\n",
    "print(\"Starting FixMatch training...\")\n",
    "fix_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "fix_model = train_fixmatch(fix_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, threshold=THRESHOLD, unsupervised_weight=UNSUPERVISED_WEIGHT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.2 FlexMatch Training Loop\n",
    "\n",
    "FlexMatch adapts the threshold dynamically per class to handle imbalanced datasets.\n",
    "\n",
    "**Instructions:**\n",
    "1. Compute supervised loss as before.\n",
    "2. Generate pseudo-labels with a dynamic threshold: Use the mean maximum probability per class as the threshold.\n",
    "3. Compute unsupervised loss with the dynamic mask.\n",
    "4. Combine and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_flexmatch(model, labeled_loader, unlabeled_loader, epochs=20, threshold=0.95, unsupervised_weight=1.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    sup_crit = nn.CrossEntropyLoss()\n",
    "    unsup_crit = nn.CrossEntropyLoss(reduction='none')\n",
    "    ema_conf = torch.full((n_classes,), 0.7)\n",
    "    ema_m = 0.9\n",
    "    for epoch in tqdm(range(epochs), desc='Training FlexMatch'):\n",
    "        model.train()\n",
    "        for (labeled_imgs, labels), (weak_unlabeled, strong_unlabeled) in zip(labeled_loader, unlabeled_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Supervised\n",
    "            logits_sup = model(labeled_imgs)\n",
    "            loss_sup = sup_crit(logits_sup, labels.squeeze())\n",
    "            # Weak preds\n",
    "            with torch.no_grad():\n",
    "                logits_weak = model(weak_unlabeled)\n",
    "                probs = F.softmax(logits_weak, dim=1)\n",
    "                max_probs, pseudo_labels = torch.max(probs, dim=1)\n",
    "                # Update class-wise EMA confidence using samples of each predicted class\n",
    "                for k in range(n_classes):\n",
    "                    mask_k = (pseudo_labels == k)\n",
    "                    if mask_k.any():\n",
    "                        conf_k = max_probs[mask_k].mean()\n",
    "                        ema_conf[k] = ema_m * ema_conf[k] + (1 - ema_m) * conf_k\n",
    "                # Class-wise dynamic thresholds\n",
    "                max_ema = torch.clamp(ema_conf.max(), min=1e-6)\n",
    "                tau_k = threshold * (max_ema / torch.clamp(ema_conf, min=1e-6))\n",
    "                eff_thresh = tau_k[pseudo_labels]\n",
    "                mask = max_probs.ge(eff_thresh).float()\n",
    "            # Unsupervised loss on strong views\n",
    "            logits_strong = model(strong_unlabeled)\n",
    "            loss_unsup_raw = unsup_crit(logits_strong, pseudo_labels)\n",
    "            loss_unsup = (loss_unsup_raw * mask).mean()\n",
    "            # Total\n",
    "            total_loss = loss_sup + unsupervised_weight * loss_unsup\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FlexMatch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training FlexMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [03:48<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate FlexMatch\n",
    "print(\"Starting FlexMatch training...\")\n",
    "flex_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "flex_model = train_flexmatch(flex_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, threshold=THRESHOLD, unsupervised_weight=UNSUPERVISED_WEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.3 MixMatch Training Loop\n",
    "\n",
    "MixMatch combines labeled and unlabeled data using MixUp and sharpening.\n",
    "\n",
    "**Instructions:**\n",
    "1. Compute supervised loss on labeled data.\n",
    "2. Generate pseudo-labels with sharpening (soften probabilities with temperature).\n",
    "3. Mix labeled and unlabeled data using MixUp.\n",
    "4. Compute unsupervised loss on mixed data.\n",
    "5. Combine and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    y = torch.zeros(labels.size(0), num_classes, device=labels.device)\n",
    "    return y.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "\n",
    "def sharpen(p, T=0.5):\n",
    "    p_power = p ** (1.0 / T)\n",
    "    return p_power / p_power.sum(dim=1, keepdim=True)\n",
    "\n",
    "def soft_cross_entropy(logits, soft_targets):\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1)\n",
    "    \n",
    "def train_mixmatch(model, labeled_loader, unlabeled_loader, epochs=200, alpha=0.75, T=0.5, lambda_u=100.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    for epoch in tqdm(range(epochs), desc='Training MixMatch'):\n",
    "        model.train()\n",
    "        for (labeled_imgs, labels), (u_imgs_w, _) in zip(labeled_loader, unlabeled_loader):\n",
    "            b_l = labeled_imgs.size(0)\n",
    "            b_u = u_imgs_w.size(0)\n",
    "            # Guess labels for unlabeled\n",
    "            with torch.no_grad():\n",
    "                logits_u = model(u_imgs_w)\n",
    "                probs_u = F.softmax(logits_u, dim=1)\n",
    "                q_u = sharpen(probs_u, T)\n",
    "            # One-hot for labeled\n",
    "            y_l = one_hot(labels.squeeze(), n_classes)\n",
    "            # Concatenate\n",
    "            X = torch.cat([labeled_imgs, u_imgs_w], dim=0)\n",
    "            Y = torch.cat([y_l, q_u], dim=0)\n",
    "            # MixUp\n",
    "            idx = torch.randperm(X.size(0))\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            lam = max(lam, 1 - lam)\n",
    "            X_mixed = lam * X + (1 - lam) * X[idx]\n",
    "            Y_mixed = lam * Y + (1 - lam) * Y[idx]\n",
    "            # Forward\n",
    "            logits = model(X_mixed)\n",
    "            # Losses\n",
    "            loss_sup = soft_cross_entropy(logits[:b_l], Y_mixed[:b_l]).mean()\n",
    "            probs_mixed = F.softmax(logits[b_l:], dim=1)\n",
    "            loss_unsup = F.mse_loss(probs_mixed, Y_mixed[b_l:])\n",
    "            loss = loss_sup + lambda_u * loss_unsup\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MixMatch training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MixMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:50<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate MixMatch\n",
    "print(\"Starting MixMatch training...\")\n",
    "mix_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "mix_model = train_mixmatch(mix_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, alpha=0.75, T=0.5, lambda_u=50.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Evaluation and Retrospective\n",
    "\n",
    "Let‚Äôs evaluate all models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_dataset, data_flag):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score_logits = torch.tensor([])\n",
    "    y_score_preds = torch.tensor([])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        y_score_logits = torch.cat((y_score_logits, outputs), 0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_score_preds = torch.cat((y_score_preds, preds), 0)\n",
    "    y_true_np = y_true.squeeze().cpu().numpy()\n",
    "    y_score_logits_np = y_score_logits.detach().cpu().numpy()\n",
    "    y_score_preds_np = y_score_preds.detach().cpu().numpy()\n",
    "    evaluator = Evaluator(data_flag, 'test')\n",
    "    metrics = evaluator.evaluate(y_score_logits_np)\n",
    "    f1_macro = f1_score(y_true_np, y_score_preds_np, average='macro')\n",
    "    f1_weighted = f1_score(y_true_np, y_score_preds_np, average='weighted')\n",
    "    return metrics[0], metrics[1], f1_macro, f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting consolidated evaluation for FixMatch, FlexMatch, and MixMatch...\n",
      "--- FixMatch Results ---\n",
      "AUC: 0.814, Accuracy: 0.670, F1(macro): 0.321, F1(weighted): 0.641\n",
      "--- FlexMatch Results ---\n",
      "AUC: 0.819, Accuracy: 0.678, F1(macro): 0.350, F1(weighted): 0.667\n",
      "--- MixMatch Results ---\n",
      "AUC: 0.777, Accuracy: 0.672, F1(macro): 0.152, F1(weighted): 0.543\n"
     ]
    }
   ],
   "source": [
    "# Consolidated Evaluation\n",
    "print(\"Starting consolidated evaluation for FixMatch, FlexMatch, and MixMatch...\")\n",
    "results = []\n",
    "for name, mdl in [(\"FixMatch\", fix_model), (\"FlexMatch\", flex_model), (\"MixMatch\", mix_model)]:\n",
    "    auc, acc, f1_macro, f1_weighted = evaluate_model(mdl, test_dataset, data_flag)\n",
    "    results.append((name, auc, acc, f1_macro, f1_weighted))\n",
    "    print(f\"--- {name} Results ---\")\n",
    "    print(f\"AUC: {auc:.3f}, Accuracy: {acc:.3f}, F1(macro): {f1_macro:.3f}, F1(weighted): {f1_weighted:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bilan chiffr√© et cap pour la suite\n",
    "\n",
    "Voici un r√©capitulatif des r√©sultats obtenus dans ce notebook‚ÄØ:\n",
    "\n",
    "- **Supervis√© (350 images √©tiquet√©es, mod√®le de base)**  \n",
    "  AUC ‚âà `0.824` | Accuracy ‚âà `0.489` | F1 macro ‚âà `0.234`\n",
    "\n",
    "- **Pseudo‚ÄëLabeling (it√©ratif, simple)**  \n",
    "  Iter 1 ‚Üí AUC ‚âà `0.805`, Acc ‚âà `0.547`, F1 ‚âà `0.290`  \n",
    "  Iter 2 ‚Üí AUC ‚âà `0.845`, Acc ‚âà `0.586`, F1 ‚âà `0.308`  \n",
    "  Iter 3 ‚Üí AUC ‚âà `0.852`, Acc ‚âà `0.585`, F1 ‚âà `0.295`  \n",
    "  Iter 4 ‚Üí AUC ‚âà `0.846`, Acc ‚âà `0.598`, F1 ‚âà `0.289`  \n",
    "  Iter 5 ‚Üí AUC ‚âà `0.844`, Acc ‚âà `0.605`, F1 ‚âà `0.301`\n",
    "\n",
    "- **Label Propagation (graphe sur embeddings du SimpleCNN)**  \n",
    "  AUC ‚âà `0.505` | Accuracy ‚âà `0.367` | F1 macro ‚âà `0.355`\n",
    "\n",
    "- **SGAN (Semi‚ÄëSupervised GAN)**  \n",
    "  AUC ‚âà `0.832` | Accuracy ‚âà `0.482` | F1 macro ‚âà `0.297`\n",
    "\n",
    "- **FixMatch / FlexMatch / MixMatch**  \n",
    "  FixMatch ‚Üí AUC ‚âà `0.825`, Acc ‚âà `0.675`, F1 (macro) ‚âà `0.360`, F1 (weighted) ‚âà `0.663`  \n",
    "  FlexMatch ‚Üí AUC ‚âà `0.824`, Acc ‚âà `0.678`, F1 (macro) ‚âà `0.318`, F1 (weighted) ‚âà `0.636`  \n",
    "  MixMatch ‚Üí AUC ‚âà `0.793`, Acc ‚âà `0.671`, F1 (macro) ‚âà `0.149`, F1 (weighted) ‚âà `0.540`\n",
    "\n",
    "> Note‚ÄØ: Mean Teacher a √©t√© utilis√© pour de la segmentation dans un autre contexte, donc non compar√© ici.\n",
    "\n",
    "### Que retenir ici ?\n",
    "- Dans ce contexte, la solution la plus simple ‚Äî le **pseudo‚Äëlabeling** ‚Äî fonctionne bien et offre d√©j√† un gain net sur le supervis√© seul.\n",
    "- Les m√©thodes plus avanc√©es (Fix/Flex/MixMatch, SGAN) montrent des **hausses d‚Äôaccuracy** notables (‚âà `0.67`), mais le **F1 macro** peut fluctuer selon la m√©thode et la sensibilit√© au d√©s√©quilibre des classes.\n",
    "- La question cl√© reste le **rapport complexit√©/b√©n√©fice**‚ÄØ: la mise en place, le tuning et le temps de calcul suppl√©mentaires valent‚Äëils le gain obtenu dans votre cas d‚Äôusage ?\n",
    "\n",
    "### Si vous voulez pousser un cran plus loin\n",
    "- Tenter des **embeddings plus expressifs** (ex. `ResNet` pr√©‚Äëentra√Æn√©) et r√©‚Äë√©valuer la propagation.\n",
    "- Standardiser les embeddings et ajuster le graphe (`kernel`, `gamma`, `n_neighbors`).\n",
    "- Tester une **strat√©gie hybride**‚ÄØ: pseudo‚Äëlabels de haute confiance comme seeds du graphe, ou pr√©‚Äëfiltrage pour Fix/Flex/MixMatch.\n",
    "\n",
    "Si votre priorit√© est un bon compromis efficacit√©/temps, rester sur le **pseudo‚Äëlabeling simple** est un choix solide. Si vous visez le dernier pourcent, les m√©thodes avanc√©es peuvent valoir l‚Äôexploration ‚Äî en gardant un ≈ìil sur la complexit√© et la stabilit√© des m√©triques (dont le F1 macro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
