{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques SSL avanc√©es - FixMatch, FlexMatch et MixMatch\n",
    "\n",
    "Bienvenue dans le chapitre avanc√© de notre parcours en SSL ! Nous avons d√©j√† explor√© le pseudo‚Äëlabeling et la r√©gularisation par consistance. Passons maintenant √† des techniques de pointe : **FixMatch**, **FlexMatch** et **MixMatch**. Ces m√©thodes combinent le meilleur du pseudo‚Äëlabeling et de la consistance pour traiter des jeux de donn√©es avec peu d‚Äô√©tiquettes, comme `DermaMNIST`.\n",
    "\n",
    "> Imaginez cela comme une mise √† niveau turbo de votre bo√Æte √† outils SSL !\n",
    "\n",
    "**Principes cl√©s :**\n",
    "- **FixMatch** : Utilise des augmentations faibles et fortes avec un seuil de confiance pour les pseudo‚Äëlabels.\n",
    "- **FlexMatch** : Am√©liore FixMatch avec un seuillage dynamique par classe, id√©al pour les donn√©es d√©s√©quilibr√©es.\n",
    "- **MixMatch** : Ajoute du m√©lange de donn√©es (ex. MixUp) pour am√©liorer la robustesse en combinant √©chantillons √©tiquet√©s et non √©tiquet√©s.\n",
    "\n",
    "**Objectifs :**\n",
    "1. Revenir √† la classification `DermaMNIST` avec 100 images √©tiquet√©es.\n",
    "2. Impl√©menter FixMatch, FlexMatch et MixMatch.\n",
    "3. Comparer les r√©sultats au baseline afin d‚Äôillustrer les avanc√©es SSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pr√©paration (configuration habituelle)\n",
    "\n",
    "Mettons en place l‚Äôenvironnement pour la classification `DermaMNIST`. Nous utiliserons 100 images √©tiquet√©es et Albumentations pour des augmentations contr√¥l√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es √©tiquet√©es : 500, Donn√©es non √©tiquet√©es : 6507\n"
     ]
    }
   ],
   "source": [
    "# Load DermaMNIST data\n",
    "data_flag = 'dermamnist'\n",
    "info = INFO[data_flag]\n",
    "n_classes = len(info['label'])\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "train_dataset = DataClass(split='train', download=True)\n",
    "test_dataset = DataClass(split='test', transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[.5], std=[.5])]), download=True)\n",
    "\n",
    "# Split into labeled (100) and unlabeled sets\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "labels_array = np.array(train_dataset.labels).flatten()\n",
    "labeled_indices, unlabeled_indices = train_test_split(all_indices, train_size=500, random_state=42, stratify=labels_array)\n",
    "\n",
    "print(f\"Donn√©es √©tiquet√©es : {len(labeled_indices)}, Donn√©es non √©tiquet√©es : {len(unlabeled_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Augmentations faibles et fortes\n",
    "\n",
    "Nous avons besoin de deux pipelines d‚Äôaugmentation : faible pour la g√©n√©ration de pseudo‚Äëlabels et forte pour accro√Ætre la robustesse √† l‚Äôentra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations initialis√©es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quillaur/jupyter_notebooks/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define weak and strong augmentations for single-channel images\n",
    "weak_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(transpose_mask=True)  # Preserve 1 channel\n",
    "])\n",
    "\n",
    "strong_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ToTensorV2(transpose_mask=True)  # Preserve 1 channel\n",
    "])\n",
    "print(\"Transformations initialis√©es\")\n",
    "\n",
    "# Custom datasets for FixMatch\n",
    "class FixMatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, transform):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img = np.array(img)  # Ensure img is [H, W] (single-channel)\n",
    "        transformed = self.transform(image=img)\n",
    "        return transformed['image'], torch.tensor(label).long()\n",
    "\n",
    "class FixMatchUnlabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, weak_transform, strong_transform):\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.dataset[idx]\n",
    "        img = np.array(img)  # Ensure img is [H, W] (single-channel)\n",
    "        weak = self.weak_transform(image=img)['image']\n",
    "        strong = self.strong_transform(image=img)['image']\n",
    "        return weak, strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mod√®les et boucles d‚Äôentra√Ænement\n",
    "\n",
    "Nous utiliserons un CNN simple et impl√©menterons trois boucles d‚Äôentra√Ænement : FixMatch, FlexMatch et MixMatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the SimpleCNN model for single-channel input\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize model, optimizer, and loss functions\n",
    "model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "supervised_criterion = nn.CrossEntropyLoss()\n",
    "unsupervised_criterion = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.1 Boucle d‚Äôentra√Ænement FixMatch\n",
    "\n",
    "Impl√©mentons l‚Äôalgorithme FixMatch pas √† pas.\n",
    "\n",
    "**Instructions :**\n",
    "1. Calculer la perte supervis√©e sur les donn√©es √©tiquet√©es.\n",
    "2. G√©n√©rer des pseudo‚Äëlabels : pr√©dire sur les augmentations faibles, calculer les probabilit√©s et cr√©er un masque pour les pr√©dictions confiantes (seuil = 0.95).\n",
    "3. Calculer la perte non supervis√©e : pr√©dire sur les augmentations fortes et appliquer le masque aux pseudo‚Äëlabels confiants.\n",
    "4. Combiner les pertes et faire la r√©tropropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeux de donn√©es cr√©√©s : √©tiquet√©=500, non √©tiquet√©=6507\n",
      "DataLoaders pr√™ts : lots √©tiquet√©s=32, non √©tiquet√©s=102\n",
      "D√©marrage de la boucle d‚Äôentra√Ænement\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "labeled_dataset = FixMatchDataset(train_dataset, labeled_indices, strong_transform)\n",
    "unlabeled_dataset = FixMatchUnlabeledDataset(train_dataset, unlabeled_indices, weak_transform, strong_transform)\n",
    "print(f\"Jeux de donn√©es cr√©√©s : √©tiquet√©={len(labeled_dataset)}, non √©tiquet√©={len(unlabeled_dataset)}\")\n",
    "\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=16, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=64, shuffle=True)\n",
    "print(f\"DataLoaders pr√™ts : lots √©tiquet√©s={len(labeled_loader)}, non √©tiquet√©s={len(unlabeled_loader)}\")\n",
    "\n",
    "print(\"D√©marrage de la boucle d‚Äôentra√Ænement\")\n",
    "# FixMatch training as a function (to unify with other methods)\n",
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, epochs=30, threshold=0.95, unsupervised_weight=1.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    sup_crit = nn.CrossEntropyLoss()\n",
    "    unsup_crit = nn.CrossEntropyLoss(reduction='none')\n",
    "    for epoch in tqdm(range(epochs), desc='Entra√Ænement FixMatch'):\n",
    "        model.train()\n",
    "        batch_iterator = zip(labeled_loader, unlabeled_loader)\n",
    "        for (labeled_imgs, labels), (weak_unlabeled, strong_unlabeled) in batch_iterator:\n",
    "            optimizer.zero_grad()\n",
    "            # Supervised loss\n",
    "            logits_sup = model(labeled_imgs)\n",
    "            loss_sup = sup_crit(logits_sup, labels.squeeze())\n",
    "            # Pseudo-labels from weak\n",
    "            with torch.no_grad():\n",
    "                logits_weak = model(weak_unlabeled)\n",
    "                probs = F.softmax(logits_weak, dim=1)\n",
    "                max_probs, pseudo_labels = torch.max(probs, dim=1)\n",
    "                mask = max_probs.ge(threshold).float()\n",
    "            # Unsupervised on strong\n",
    "            logits_strong = model(strong_unlabeled)\n",
    "            loss_unsup_raw = unsup_crit(logits_strong, pseudo_labels)\n",
    "            loss_unsup = (loss_unsup_raw * mask).mean()\n",
    "            # Total\n",
    "            total_loss = loss_sup + unsupervised_weight * loss_unsup\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l‚Äôentra√Ænement FixMatch‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entra√Ænement FixMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:47<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "THRESHOLD = 0.95\n",
    "UNSUPERVISED_WEIGHT = 1.0\n",
    "\n",
    "print(\"D√©but de l‚Äôentra√Ænement FixMatch‚Ä¶\")\n",
    "fix_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "fix_model = train_fixmatch(fix_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, threshold=THRESHOLD, unsupervised_weight=UNSUPERVISED_WEIGHT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.2 Boucle d‚Äôentra√Ænement FlexMatch\n",
    "\n",
    "FlexMatch adapte le seuil dynamiquement par classe pour g√©rer les jeux de donn√©es d√©s√©quilibr√©s.\n",
    "\n",
    "**Instructions :**\n",
    "1. Calculer la perte supervis√©e comme pr√©c√©demment.\n",
    "2. G√©n√©rer des pseudo‚Äëlabels avec un seuil dynamique : utiliser la probabilit√© maximale moyenne par classe comme seuil.\n",
    "3. Calculer la perte non supervis√©e avec le masque dynamique.\n",
    "4. Combiner et r√©tropropager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flexmatch(model, labeled_loader, unlabeled_loader, epochs=20, threshold=0.95, unsupervised_weight=1.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    sup_crit = nn.CrossEntropyLoss()\n",
    "    unsup_crit = nn.CrossEntropyLoss(reduction='none')\n",
    "    ema_conf = torch.full((n_classes,), 0.7)\n",
    "    ema_m = 0.9\n",
    "    for epoch in tqdm(range(epochs), desc='Entra√Ænement FlexMatch'):\n",
    "        model.train()\n",
    "        for (labeled_imgs, labels), (weak_unlabeled, strong_unlabeled) in zip(labeled_loader, unlabeled_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Supervised\n",
    "            logits_sup = model(labeled_imgs)\n",
    "            loss_sup = sup_crit(logits_sup, labels.squeeze())\n",
    "            # Weak preds\n",
    "            with torch.no_grad():\n",
    "                logits_weak = model(weak_unlabeled)\n",
    "                probs = F.softmax(logits_weak, dim=1)\n",
    "                max_probs, pseudo_labels = torch.max(probs, dim=1)\n",
    "                # Update class-wise EMA confidence using samples of each predicted class\n",
    "                for k in range(n_classes):\n",
    "                    mask_k = (pseudo_labels == k)\n",
    "                    if mask_k.any():\n",
    "                        conf_k = max_probs[mask_k].mean()\n",
    "                        ema_conf[k] = ema_m * ema_conf[k] + (1 - ema_m) * conf_k\n",
    "                # Class-wise dynamic thresholds\n",
    "                max_ema = torch.clamp(ema_conf.max(), min=1e-6)\n",
    "                tau_k = threshold * (max_ema / torch.clamp(ema_conf, min=1e-6))\n",
    "                eff_thresh = tau_k[pseudo_labels]\n",
    "                mask = max_probs.ge(eff_thresh).float()\n",
    "            # Unsupervised loss on strong views\n",
    "            logits_strong = model(strong_unlabeled)\n",
    "            loss_unsup_raw = unsup_crit(logits_strong, pseudo_labels)\n",
    "            loss_unsup = (loss_unsup_raw * mask).mean()\n",
    "            # Total\n",
    "            total_loss = loss_sup + unsupervised_weight * loss_unsup\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l‚Äôentra√Ænement FlexMatch‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entra√Ænement FlexMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:21<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate FlexMatch\n",
    "print(\"D√©but de l‚Äôentra√Ænement FlexMatch‚Ä¶\")\n",
    "flex_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "flex_model = train_flexmatch(flex_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, threshold=THRESHOLD, unsupervised_weight=UNSUPERVISED_WEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 2.3 Boucle d‚Äôentra√Ænement MixMatch\n",
    "\n",
    "MixMatch combine donn√©es √©tiquet√©es et non √©tiquet√©es via MixUp et un ¬´ sharpening ¬ª des probabilit√©s.\n",
    "\n",
    "**Instructions :**\n",
    "1. Calculer la perte supervis√©e sur les donn√©es √©tiquet√©es.\n",
    "2. G√©n√©rer des pseudo‚Äëlabels avec sharpening (adoucir/affiner les probabilit√©s avec une temp√©rature).\n",
    "3. M√©langer donn√©es √©tiquet√©es et non √©tiquet√©es avec MixUp.\n",
    "4. Calculer la perte non supervis√©e sur les donn√©es m√©lang√©es.\n",
    "5. Combiner et r√©tropropager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    y = torch.zeros(labels.size(0), num_classes, device=labels.device)\n",
    "    return y.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "\n",
    "def sharpen(p, T=0.5):\n",
    "    p_power = p ** (1.0 / T)\n",
    "    return p_power / p_power.sum(dim=1, keepdim=True)\n",
    "\n",
    "def soft_cross_entropy(logits, soft_targets):\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * log_probs).sum(dim=1)\n",
    "    \n",
    "def train_mixmatch(model, labeled_loader, unlabeled_loader, epochs=200, alpha=0.75, T=0.5, lambda_u=100.0):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    for epoch in tqdm(range(epochs), desc='Entra√Ænement MixMatch'):\n",
    "        model.train()\n",
    "        for (labeled_imgs, labels), (u_imgs_w, _) in zip(labeled_loader, unlabeled_loader):\n",
    "            b_l = labeled_imgs.size(0)\n",
    "            b_u = u_imgs_w.size(0)\n",
    "            # Guess labels for unlabeled\n",
    "            with torch.no_grad():\n",
    "                logits_u = model(u_imgs_w)\n",
    "                probs_u = F.softmax(logits_u, dim=1)\n",
    "                q_u = sharpen(probs_u, T)\n",
    "            # One-hot for labeled\n",
    "            y_l = one_hot(labels.squeeze(), n_classes)\n",
    "            # Concatenate\n",
    "            X = torch.cat([labeled_imgs, u_imgs_w], dim=0)\n",
    "            Y = torch.cat([y_l, q_u], dim=0)\n",
    "            # MixUp\n",
    "            idx = torch.randperm(X.size(0))\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            lam = max(lam, 1 - lam)\n",
    "            X_mixed = lam * X + (1 - lam) * X[idx]\n",
    "            Y_mixed = lam * Y + (1 - lam) * Y[idx]\n",
    "            # Forward\n",
    "            logits = model(X_mixed)\n",
    "            # Losses\n",
    "            loss_sup = soft_cross_entropy(logits[:b_l], Y_mixed[:b_l]).mean()\n",
    "            probs_mixed = F.softmax(logits[b_l:], dim=1)\n",
    "            loss_unsup = F.mse_loss(probs_mixed, Y_mixed[b_l:])\n",
    "            loss = loss_sup + lambda_u * loss_unsup\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l‚Äôentra√Ænement MixMatch‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entra√Ænement MixMatch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:20<00:00,  2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate MixMatch\n",
    "print(\"D√©but de l‚Äôentra√Ænement MixMatch‚Ä¶\")\n",
    "mix_model = SimpleCNN(in_channels=3, num_classes=n_classes)\n",
    "mix_model = train_mixmatch(mix_model, labeled_loader, unlabeled_loader, epochs=EPOCHS, alpha=0.75, T=0.5, lambda_u=50.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. √âvaluation finale et r√©trospective\n",
    "\n",
    "√âvaluons tous les mod√®les et comparons leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_dataset, data_flag):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score_logits = torch.tensor([])\n",
    "    y_score_preds = torch.tensor([])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        y_true = torch.cat((y_true, labels), 0)\n",
    "        y_score_logits = torch.cat((y_score_logits, outputs), 0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        y_score_preds = torch.cat((y_score_preds, preds), 0)\n",
    "    y_true_np = y_true.squeeze().cpu().numpy()\n",
    "    y_score_logits_np = y_score_logits.detach().cpu().numpy()\n",
    "    y_score_preds_np = y_score_preds.detach().cpu().numpy()\n",
    "    evaluator = Evaluator(data_flag, 'test')\n",
    "    metrics = evaluator.evaluate(y_score_logits_np)\n",
    "    f1_macro = f1_score(y_true_np, y_score_preds_np, average='macro')\n",
    "    f1_weighted = f1_score(y_true_np, y_score_preds_np, average='weighted')\n",
    "    return metrics[0], metrics[1], f1_macro, f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but de l‚Äô√©valuation consolid√©e pour FixMatch, FlexMatch et MixMatch‚Ä¶\n",
      "--- R√©sultats FixMatch ---\n",
      "AUC : 0.816, Accuracy : 0.685, F1 (macro) : 0.301, F1 (pond√©r√©) : 0.635\n",
      "--- R√©sultats FlexMatch ---\n",
      "AUC : 0.821, Accuracy : 0.694, F1 (macro) : 0.332, F1 (pond√©r√©) : 0.659\n",
      "--- R√©sultats MixMatch ---\n",
      "AUC : 0.804, Accuracy : 0.672, F1 (macro) : 0.162, F1 (pond√©r√©) : 0.542\n"
     ]
    }
   ],
   "source": [
    "# Consolidated Evaluation\n",
    "print(\"D√©but de l‚Äô√©valuation consolid√©e pour FixMatch, FlexMatch et MixMatch‚Ä¶\")\n",
    "results = []\n",
    "for name, mdl in [(\"FixMatch\", fix_model), (\"FlexMatch\", flex_model), (\"MixMatch\", mix_model)]:\n",
    "    auc, acc, f1_macro, f1_weighted = evaluate_model(mdl, test_dataset, data_flag)\n",
    "    results.append((name, auc, acc, f1_macro, f1_weighted))\n",
    "    print(f\"--- R√©sultats {name} ---\")\n",
    "    print(f\"AUC : {auc:.3f}, Accuracy : {acc:.3f}, F1 (macro) : {f1_macro:.3f}, F1 (pond√©r√©) : {f1_weighted:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bilan chiffr√© et cap pour la suite\n",
    "\n",
    "Voici un r√©capitulatif des r√©sultats obtenus dans ce notebook‚ÄØ:\n",
    "\n",
    "- **Supervis√© (350 images √©tiquet√©es, mod√®le de base)**  \n",
    "  AUC ‚âà `0.824` | Accuracy ‚âà `0.489` | F1 macro ‚âà `0.234`\n",
    "\n",
    "- **Pseudo‚ÄëLabeling (it√©ratif, simple)**  \n",
    "  Iter 1 ‚Üí AUC ‚âà `0.805`, Acc ‚âà `0.547`, F1 ‚âà `0.290`  \n",
    "  Iter 2 ‚Üí AUC ‚âà `0.845`, Acc ‚âà `0.586`, F1 ‚âà `0.308`  \n",
    "  Iter 3 ‚Üí AUC ‚âà `0.852`, Acc ‚âà `0.585`, F1 ‚âà `0.295`  \n",
    "  Iter 4 ‚Üí AUC ‚âà `0.846`, Acc ‚âà `0.598`, F1 ‚âà `0.289`  \n",
    "  Iter 5 ‚Üí AUC ‚âà `0.844`, Acc ‚âà `0.605`, F1 ‚âà `0.301`\n",
    "\n",
    "- **Label Propagation (graphe sur embeddings du SimpleCNN)**  \n",
    "  AUC ‚âà `0.505` | Accuracy ‚âà `0.367` | F1 macro ‚âà `0.355`\n",
    "\n",
    "- **SGAN (Semi‚ÄëSupervised GAN)**  \n",
    "  AUC ‚âà `0.832` | Accuracy ‚âà `0.482` | F1 macro ‚âà `0.297`\n",
    "\n",
    "- **FixMatch / FlexMatch / MixMatch**  \n",
    "  FixMatch ‚Üí AUC ‚âà `0.825`, Acc ‚âà `0.675`, F1 (macro) ‚âà `0.360`, F1 (weighted) ‚âà `0.663`  \n",
    "  FlexMatch ‚Üí AUC ‚âà `0.824`, Acc ‚âà `0.678`, F1 (macro) ‚âà `0.318`, F1 (weighted) ‚âà `0.636`  \n",
    "  MixMatch ‚Üí AUC ‚âà `0.793`, Acc ‚âà `0.671`, F1 (macro) ‚âà `0.149`, F1 (weighted) ‚âà `0.540`\n",
    "\n",
    "> Note‚ÄØ: Mean Teacher a √©t√© utilis√© pour de la segmentation dans un autre contexte, donc non compar√© ici.\n",
    "\n",
    "### Que retenir ici ?\n",
    "- Dans ce contexte, la solution la plus simple ‚Äî le **pseudo‚Äëlabeling** ‚Äî fonctionne bien et offre d√©j√† un gain net sur le supervis√© seul.\n",
    "- Les m√©thodes plus avanc√©es (Fix/Flex/MixMatch, SGAN) montrent des **hausses d‚Äôaccuracy** notables (‚âà `0.67`), mais le **F1 macro** peut fluctuer selon la m√©thode et la sensibilit√© au d√©s√©quilibre des classes.\n",
    "- La question cl√© reste le **rapport complexit√©/b√©n√©fice**‚ÄØ: la mise en place, le tuning et le temps de calcul suppl√©mentaires valent‚Äëils le gain obtenu dans votre cas d‚Äôusage ?\n",
    "\n",
    "### Si vous voulez pousser un cran plus loin\n",
    "- Tenter des **embeddings plus expressifs** (ex. `ResNet` pr√©‚Äëentra√Æn√©) et r√©‚Äë√©valuer la propagation.\n",
    "- Standardiser les embeddings et ajuster le graphe (`kernel`, `gamma`, `n_neighbors`).\n",
    "- Tester une **strat√©gie hybride**‚ÄØ: pseudo‚Äëlabels de haute confiance comme seeds du graphe, ou pr√©‚Äëfiltrage pour Fix/Flex/MixMatch.\n",
    "\n",
    "Si votre priorit√© est un bon compromis efficacit√©/temps, rester sur le **pseudo‚Äëlabeling simple** est un choix solide. Si vous visez le dernier pourcent, les m√©thodes avanc√©es peuvent valoir l‚Äôexploration ‚Äî en gardant un ≈ìil sur la complexit√© et la stabilit√© des m√©triques (dont le F1 macro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
